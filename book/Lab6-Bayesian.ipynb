{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Modelling\n",
    "\n",
    "This notebook is part of a larger effort to offer an approachable introduction to models of the mind and the brain for the course “Foundations of Neural and Cognitive Modelling”, offered at the University of Amsterdam by [Jelle (aka Willem) Zuidema](https://staff.fnwi.uva.nl/w.zuidema/). The notebook in this present form is the result of the combined work of Iris Proff, [Marianne de Heer Kloots](http://mdhk.net/), and [Simone Astarita](https://www.linkedin.com/in/simone-astarita-4499b11b5/).\n",
    "\n",
    "### Instructions\n",
    "Please hand in the following:\n",
    "- A copy of this notebook with the **code** and results of running the code filled in the required sections. The sections to complete all start as follows:\n",
    "\n",
    "<code>### YOUR CODE HERE ###</code>\n",
    "\n",
    "- A separate pdf file with the answers to the **homework exercises**. These can be identified by the following formatting, where **n** is the number of points (out of 10) that question **m** is worth:\n",
    "<br>\n",
    "\n",
    ">***Homework exercise m***: question(s) **(npt)**.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this lab we are looking at the basics of Bayesian modelling: probability distributions, priors, posteriors, and Bayes’ rule.\n",
    "\n",
    "### 1. Probability distributions\n",
    "\n",
    "Probability distributions are used to describe random processes, such as tossing a coin or randomly sampling people from a population. A probability distribution is a function that maps all possible values of a random process to their respective probabilities. Probability distributions can take many different shapes. We will discuss some common probability distributions and how to work with them.\n",
    "\n",
    "#### 1.1 Uniform distribution\n",
    "\n",
    "The simplest probability distribution is the uniform distribution: each value of a certain range occurs with the same probability. We can use <code>np.random.uniform()</code> to sample from the uniform distribution. \n",
    "\n",
    "> ***Homework exercise 1:***  The cell below samples values from a uniform distribution between $0$ and $1$ and plots them as a histogram. What does the cumulative histogram express? What happens if you change the number of sampled values and why? **(2pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# sample 10 values from an uniform distribution, going from 0 to 1\n",
    "x = np.random.uniform(0,1,10)\n",
    "n_bins = 20\n",
    "\n",
    "# plot histogram\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "n, bins, patches = ax.hist(x, n_bins, cumulative=False, label='Empirical',rwidth=0.9)\n",
    "ax.set_title('Histogram')\n",
    "\n",
    "# cumulative histogram\n",
    "fig, ax = plt.subplots()\n",
    "n, bins, patches = ax.hist(x, n_bins, cumulative=True, label='Empirical', color ='lightgreen',rwidth=0.9)\n",
    "ax.set_title('Culumative histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Geometric distibution\n",
    "\n",
    "We now sample from the geometric distribution which is given by:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p(X|\\theta)= \\theta(1-\\theta)^{X-1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The distribution expresses the probability of tossing a coin $x$ times, until head appears for the first time. The parameter $\\theta$ is the bias of the coin: a fair coin has a bias of $0.5$. In that case, both outcomes are equally likely.\n",
    "\n",
    "Given a value $\\theta$ (“theta”), flipping the coin until head comes up can be simulated using a for-loop. \n",
    "\n",
    "> Play around with parameter theta and observe the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0         # x counts number of coin tosses\n",
    "head = False  # head tracks if we already reached outcome \"head\"\n",
    "theta = 0.5   # define the bias\n",
    "\n",
    "# repeat until we throw \"head\"\n",
    "while not head:\n",
    "    x = x+1\n",
    "    head = np.random.uniform(0,1,1) < theta # throw the coin with bias theta\n",
    "    \n",
    "print(x,'coin tosses until head')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scenario gets more interesting if the bias $\\theta$ itself is not a fixed value, but drawn from a probability distribution. This distribution over $\\theta$ is the **prior distribution**: it biases the outcomes $X$ into a certain direction.\n",
    "\n",
    "The function <code>coin_tosses()</code> in the cell below samples a pair $(\\theta, X)$ from the geometric distribution (as we did above, but now in a function). \n",
    "\n",
    "> Add a line of code to sample a value for $\\theta$ from a uniform distribution between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_tosses():\n",
    "    \n",
    "    ### YOUR CODE HERE ### \n",
    "    # draw theta from a uniform distribution between 0 and 1\n",
    "    theta = ...\n",
    "\n",
    "    x = 0\n",
    "    head = False\n",
    "    \n",
    "    # repeat until we throw \"head\"\n",
    "    while not head:\n",
    "        x = x+1\n",
    "        head = np.random.uniform(0,1,1) < theta # throw the coin with bias theta\n",
    "        \n",
    "    return theta, x # return bias and number of coin tosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses the function <code>coin_tosses</code> to draw 200 $(\\theta,x)$ pairs and plots them in a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "n = 200\n",
    "xs = np.zeros([1,n])\n",
    "thetas = np.zeros([1,n])\n",
    "\n",
    "# draw 200 (theta, x) pairs using function coin_tosses\n",
    "for i in range(0,n):\n",
    "    thetas[0,i], xs[0,i] = coin_tosses() # store theta and x in the arrays \"thetas\" and \"xs\"\n",
    "\n",
    "# make a scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(thetas,xs,marker = '*')\n",
    "ax.set_xlabel('theta')\n",
    "ax.set_ylabel('number coin tosses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have just played around with your first **hierarchical Bayesian model**, where one stochastic process (selecting $\\theta$) determines the parameters of another stochastic process (producing $X$s).\n",
    "\n",
    "#### 1.3 Sampling using Python\n",
    "\n",
    "Next to the uniform distribution we have been using already, Python has built-in sampling functions for many standard probability distributions, for instance:\n",
    "\n",
    "<code>numpy.random.binomial()</code> \n",
    "<code>numpy.random.normal()</code> \n",
    "<code>numpy.random.poisson()</code> \n",
    "<code>numpy.random.geometric()</code>\n",
    "\n",
    "You can use these functions to draw samples from these distributions.\n",
    "\n",
    "> ***Homework exercise 2:*** Reproduce the scatterplot we created in the cell above by using the function <code>numpy.random.geometric()</code> instead of a for-loop. You need to generate a sequence of $200$ random values for $\\theta$. Do the plots look more or less the same? **(0.5pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# create a sequence of 200 thetas\n",
    "my_thetas = ...\n",
    "\n",
    "# create a sequence of 200 coin tosses\n",
    "my_xs = np.random.geometric(my_thetas)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(my_thetas,my_xs,marker = '*')\n",
    "ax.set_xlabel('theta')\n",
    "ax.set_ylabel('number coin tosses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deriving the posterior\n",
    "\n",
    "To develop a bit of an intuition about how probability distributions let us model interesting phenomena in cognitive science, we consider the slightly more complex **Poisson distribution** and use it as a model of neural spike trains.\n",
    "\n",
    "Neurons are believed to encode relevant information in the firing rate of a spike train. For instance, the brightness of a visual stimulus $s$ can be encoded through some function $r=f(s)$ that yields the rate $r$ of the resulting spike train.\n",
    "Such spike trains can be modeled with a Poisson distribution, where spikes are generated randomly with rate $r$; we will assume that the neuron has a constant rate of firing. The distribution of the spike count $X$ in a time interval of length $T$ is then given by:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    P(X|r) = \\frac{(rT)^X}{X!}e^{-rT}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "We call this probability distribution the **likelihood** or **model evidence**. It gives us the probability of observing data $X$ given an hypothesis: in this case, a spike rate of $r$.\n",
    "\n",
    "\n",
    "> ***Homework exercise 3:*** Pick some values for $r$ and generate plots for $P(X)$ for these values using <code>np.random.poisson()</code>. You need to generate a sequence of $20000$ random values from the poisson distribution. Where does this function have most of its probability mass? We assume $T = 1$. **(0.5pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# define spike rate\n",
    "r = ...\n",
    "\n",
    "# draw 20000 samples from the poisson distribution\n",
    "xs_poisson = \n",
    "\n",
    "n_bins = 10\n",
    "\n",
    "# histogram\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "n, bins, patches = ax.hist(xs_poisson, n_bins,rwidth=0.9)\n",
    "plt.xlabel('number of spikes')\n",
    "plt.ylabel('samples')\n",
    "plt.legend(['r = {}'.format(r)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look a bit closer at the concepts of prior and posterior probabilities by looking at an imaginary neuron that responds differently to different stimuli. Say our imaginary neuron will respond to stimulus $A$ with a firing rate of $r=3$, whereas stimulus $B$ will elicit a spiking response with rate $r=8$. We will try to infer what stimulus is being presented to the neuron by looking at the response of the neuron. A priori, stimulus $A$ is a lot more likely to occur than stimulus $B$: $P(A) = 0.7$, whereas $P(B) = 0.3$; we call the probability distribution over $A$ and $B$ the **prior**.\n",
    "\n",
    "We now measure the response of our imaginary neuron; over a period $T$ we measure $X$ spikes. We call this the **data**. \n",
    "\n",
    "Bayes’ rule allows us to use data, likelihood and prior to compute the **posterior probability** of each hypothesis ($A$: $r=3$; $B$: $r=8$) given the data:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "P(r | X) = \\frac{P(X | r) P(r)}{\\sum_{r'}P(r')P(X|r')}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The posterior is the probability of our hypothesis, given the data that we observed. This is an extremly useful value, because it allows to directly compare different hypothesis about the data against each other. The denominator is a normalization term and it is the same for each hypothesis. Given that $P(A) + P(B) = 1$, the denominator, in our case, can be rewritten as the sum of the two numerators, the one for $A$ and the one for $B$.\n",
    "\n",
    "With the following function <code>eval_poiss</code> we can compute the likelihood $P(X|r)$ for the poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the poisson distribution at one place, thus computes P(data|r)\n",
    "\n",
    "def eval_poiss(r, data):\n",
    "    p = r**data/math.factorial(data)*math.exp(-r)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Homework exercise 4:*** Compute the posterior probability for each of the two rates ($A$: $r=3$, $P(A) = 0.7$; $B$: $r=8$, $P(B) = 0.7$) given an observation $X=6$ in the cell below. Which stimulus has most likely caused the observed spikes? How can you explain how close the result is? **(1pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "data = ...\n",
    "priorA = ...\n",
    "priorB = ...\n",
    "rA = ...\n",
    "rB = ...\n",
    "\n",
    "numerator_A = priorA * eval_poiss(rA, data)\n",
    "numerator_B = priorB * eval_poiss(rB, data)\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "denominator = ...\n",
    "posterior_A = ...\n",
    "posterior_B = ...\n",
    "###\n",
    "\n",
    "print('posterior A =', posterior_A)\n",
    "print('posterior B =', posterior_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Homework exercise 5:*** The following cell creates a bar plot of the posterior probability over $r$ that would result from each of the observations $X=1$ to $X=10$. What do you observe? Explain it. **(2pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "posteriors_A = np.zeros([1,11])\n",
    "posteriors_B = np.zeros([1,11])\n",
    "\n",
    "# compute posterior for each observation 1 - 10 and each hypothesis A and B\n",
    "for data in range(1,11):\n",
    "    numerator_A = priorA*eval_poiss(rA,data)\n",
    "    numerator_B = priorB*eval_poiss(rB,data)\n",
    "    denominator = numerator_A+numerator_B\n",
    "    posteriors_A[0,data-1] = numerator_A/denominator\n",
    "    posteriors_B[0,data-1] = numerator_B/denominator\n",
    "\n",
    "# plot \n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(np.arange(1,12,1),posteriors_A[0,:])\n",
    "plt.bar(np.arange(1,12,1),posteriors_B[0,:], width = 0.4)\n",
    "plt.xlabel('number of spikes')\n",
    "plt.ylabel('probability')\n",
    "plt.legend(['stimulus A','stimulus B'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
