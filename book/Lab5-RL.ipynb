{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwxp7dDmzIYV"
   },
   "source": [
    "# 5. Reinforcement Learning\n",
    "\n",
    "This notebook is part of a larger effort to offer an approachable introduction to models of the mind and the brain for the course “Foundations of Neural and Cognitive Modelling”, offered at the University of Amsterdam by [Jelle (aka Willem) Zuidema](https://staff.fnwi.uva.nl/w.zuidema/). The notebook in this present form is the result of the combined work of Iris Proff, [Marianne de Heer Kloots](http://mdhk.net/), and [Simone Astarita](https://www.linkedin.com/in/simone-astarita-4499b11b5/).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "The following instructions apply if and only if you are a student taking the course “Foundations of Neural and Cognitive Modelling” at the University of Amsterdam (Semester 1, Period 2, Year 2022).\n",
    "\n",
    "Submit your solutions on Canvas by Tuesday 6th Decemeber 18:00. Please hand in the following:\n",
    "- A copy of this notebook with the **code** and results of running the code filled in the required sections. The sections to complete all start as follows:\n",
    "\n",
    "<code>### YOUR CODE HERE ###</code>\n",
    "\n",
    "- A separate pdf file with the answers to the **homework exercises**. These can be identified by the following formatting, where **n** is the number of points (out of 10) that question **m** is worth:\n",
    "<br>\n",
    "\n",
    ">***Homework exercise m***: question(s) **(npt)**.\n",
    "\n",
    "Note that this notebook is structred a little differently: you first complete the coding portion and then answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4KpRtZKDdar"
   },
   "source": [
    "### Introduction\n",
    "This lab will guide you through a reinforcement learning simulation. We will simulate an agent navigating through a maze similar to the one depicted below. The red dot is the initial position of the agent, $s_1$. The green boxes are terminal states. The stars indicate rewards. \n",
    "\n",
    "Each position in the maze is one possible state, thus there are 16 states, which we will index line by line, such that the upper left corner is state 0 and the lower right corner is state 15. Hence, state 1 and state 11 are terminal states. There are four possible action directions, left (0), up (1), right (2) and down (3). Performing an action from a state is only possible if there is no border in the way.\n",
    "\n",
    "We assume that the setup of the maze is unknown to the agent. We will implement a simple Q-learning algorithm to model how the agent learns which path to take in the maze. \n",
    "\n",
    "![title](https://raw.githubusercontent.com/clclab/FNCM/main/book/Lab5-maze.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt7bTojtAn1G"
   },
   "source": [
    "### 1. Updating the Q-values\n",
    "\n",
    "As the agent navigates through the maze, it builds up an estimation of the utility of each state-action pair. This estimation is represented in a 16x4 matrix $Q$. Each time the agent takes a step, the Q-value of the corresponding state-action pair is updated. Specifically, when moving from state $s$ to state $s'$ with action $a$ and obtaining reward $R$, $Q(s,a)$ is updated according to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Q_{t+1}(s,a)=Q_t(s,a)+\\alpha*\\delta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\delta$ is the prediction error, defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   \\delta = R + \\gamma * max_{a'}(Q(s',a'))-Q(s,a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate and $\\gamma$ is the temporal discount factor. $max_{a'}(Q(s',a'))$ refers to the highest Q-value of state $s'$. $Q(s,a)$ is updated proportionally to the size of the prediction error – the greater the prediction error, the more the agent learns.\n",
    "\n",
    ">Complete the function that updates the Q-values in the cell below. \n",
    "\n",
    "Hint: use the function <code>np.nanmax()</code> to find the maximum of an array while ignoring NaN entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eWOzBI4An1I"
   },
   "outputs": [],
   "source": [
    "def update_Q(a,s,s1,R,Q, gamma, alpha):\n",
    "    \"\"\"\n",
    "    Function to update Q values.\n",
    "    \n",
    "    Input:\n",
    "      a -- action (integer between 0 and 3)\n",
    "      s -- state (integer between 0 and 15)\n",
    "      s1 -- new state (integer between 0 and 15)\n",
    "      R -- reward value\n",
    "      Q -- (16, 4) array with Q-values for each (s, a) pair\n",
    "      gamma -- temporal discount value\n",
    "      alpha -- learning rate\n",
    "      \n",
    "    Output:\n",
    "      Q[s, a] -- updated Q-value\n",
    "      pred_error -- prediction error (delta)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    # compute prediction error\n",
    "    pred_error = \n",
    "    \n",
    "    # update Q value\n",
    "    Q[s,a] = \n",
    "        \n",
    "    return Q[s,a], pred_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQCvk8g6An1L"
   },
   "source": [
    "### 2. Softmax action selection\n",
    "\n",
    "The second component of our Q-learning algorithm is an action selection function, that receives the Q-values of the current state as an input and returns an action to be taken. We will implement a softmax action selection function, that assigns probabilities to each action $a_i$ of a given state $s$, depending on its Q-value $q_i$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(q_i|s) = \\frac{e^{\\frac{q_i}{\\tau}}}{\\sum_A{e^{\\frac{q_i}{\\tau}}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, $\\tau > 0$ is the so called temperature parameter. If $\\tau$ is close to $0$, the algorithm most likely selects the action with the highest Q-value (i.e. it makes a *greedy* choice). For $\\tau \\rightarrow \\infty$, the algorithm *randomly* selects one of the actions, irrespective of their Q-value. The softmax function is implemented in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHOqEdJbAn1L"
   },
   "outputs": [],
   "source": [
    "def softmax_act_select(Q, tau):\n",
    "    \"\"\"\n",
    "    Softmax function for action selection.\n",
    "    \n",
    "    Input:\n",
    "      Q -- (16, 4) array with Q-values for each (s, a) pair\n",
    "      tau -- temperature parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    Qs = Q[~np.isnan(Q)] # get valid actions\n",
    "    actions =np.where(~np.isnan(Q)) # get valid action indices\n",
    "    actions = actions[0]\n",
    "    \n",
    "    # compute probabliities for each action\n",
    "    x = np.zeros(Qs.size); p = np.zeros(Qs.size);\n",
    "\n",
    "    for i in range(Qs.size):\n",
    "        x[i] = np.exp(Qs[i]/tau)/sum(np.exp(Qs)/tau)\n",
    "\n",
    "    p = x/sum(x)\n",
    "    \n",
    "    # choose action\n",
    "    a = np.random.choice(actions, p = p)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHKE5kmJAn1M"
   },
   "source": [
    "### 3. Running the simulation\n",
    "\n",
    "Now we are ready to run the simulation. The code below sets values for our model parameter and implements the maze structure. Then it runs the simulation. Our agent has to solve the maze 100 times (you can change this number). In each trial, it starts in the initial state and can move freely around in the maze until it reaches one of the terminal states. \n",
    "\n",
    "We store the number of steps the agent takes in each trial, the Q-values after each trial, the prediction errors and visited state of each step and which terminal state was reached in each trial. These results are plotted in the lower cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owhsR9idAn1M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### set parameter values\n",
    "\n",
    "alpha = 0.1   # learning rate, 0 < alpha < 1\n",
    "gamma = 0.5   # temporal discount factor, 0 <= gamma <=1\n",
    "tau = 0.2     # temperature of softmax action selection, tau > 0\n",
    "trials = 100  # number of times the agent has to solve the maze\n",
    "\n",
    "### implement maze structure\n",
    "\n",
    "# initialize Q(s,a)\n",
    "Q = np.zeros([16,4])\n",
    "Q.fill(np.nan)  # array of nans\n",
    "\n",
    "# zeros for each possible action\n",
    "Q[0,2] = 0; Q[0,3] = 0; Q[1,0] = 0; Q[2,2] = 0; Q[2,3] = 0; Q[3,0] = 0\n",
    "Q[4,1] = 0; Q[4,3] = 0; Q[5,2] = 0; Q[6,0] = 0; Q[6,1] = 0; Q[6,2] = 0; Q[6,3] = 0; Q[7,0] = 0; Q[7,3] = 0\n",
    "Q[8,1] = 0; Q[8,2] = 0; Q[8,3] = 0; Q[9,0] = 0; Q[9,2] = 0; Q[10,0] = 0; Q[10,1] = 0; Q[10,3]= 0; Q[11,1]=0\n",
    "Q[12,1] = 0; Q[12,2] = 0; Q[13,0] = 0; Q[14,1] = 0; Q[14,2] = 0; Q[15,0] = 0\n",
    "\n",
    "# terminal and initial states\n",
    "s_term = [1,11]\n",
    "s_init = 13\n",
    "\n",
    "# rewards\n",
    "Rs = np.zeros([16,1])\n",
    "Rs[1] = 5; Rs[11] = 5\n",
    "\n",
    "### initialize variables to store data \n",
    "steps = np.zeros([trials,1])\n",
    "s_term_meta = np.zeros([trials,1])\n",
    "Q_meta = np.zeros([trials,16,4])\n",
    "pred_error_meta = [];\n",
    "visited_states = []\n",
    "\n",
    "states = np.arange(16).reshape(4,4)\n",
    "\n",
    "### run simulation\n",
    "\n",
    "for trial in range(trials):\n",
    "    \n",
    "    # place agent in initial state\n",
    "    s = s_init\n",
    "    \n",
    "    # store initial state\n",
    "    visited_states.append([s_init])\n",
    "    \n",
    "    # store Q values\n",
    "    Q_meta[trial,:,:] = Q\n",
    "    \n",
    "    # continue until in terminal state\n",
    "    while not(s in s_term):\n",
    "        # print(s)\n",
    "        # choose action\n",
    "        a = softmax_act_select(Q[s], tau)\n",
    "\n",
    "        # observe new state\n",
    "        # left\n",
    "        if a == 0:\n",
    "            s1 = s-1\n",
    "        # up\n",
    "        elif a == 1:\n",
    "            s1= s-4\n",
    "        # right\n",
    "        elif a == 2:\n",
    "            s1 = s+1\n",
    "        # down\n",
    "        else:\n",
    "            s1 = s+4\n",
    "\n",
    "        # observe R\n",
    "        R = Rs[s1]\n",
    "\n",
    "        # update Q\n",
    "        Q[s,a], pred_error = update_Q(a,s,s1,R,Q, gamma, alpha)\n",
    "\n",
    "        # update state\n",
    "        s = s1\n",
    "    \n",
    "        # count steps\n",
    "        steps[trial] += 1\n",
    "        \n",
    "        # store prediction error \n",
    "        pred_error_meta.append(pred_error)\n",
    "        \n",
    "        # store visited state\n",
    "        visited_states[trial].append(s1)\n",
    "    \n",
    "    # store terminal state\n",
    "    s_term_meta[trial] = s1\n",
    "        \n",
    "\n",
    "### plot some results\n",
    "\n",
    "# plot final Q-values for each state\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(Q)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('final Q-values')\n",
    "plt.xticks([0,1,2,3], ['left', 'up', 'right', 'down'])\n",
    "plt.yticks(range(16))\n",
    "plt.xlabel('actions')\n",
    "plt.ylabel('states')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###### helper funtions######\n",
    "\n",
    "# function to get coordinates for given state (used for plotting)\n",
    "def xy(s):\n",
    "    x = np.where(states == s)[1][0]\n",
    "    y = states.shape[0] - 1 - np.where(states == s)[0][0]\n",
    "    return (x, y)\n",
    "\n",
    "# function to plot visited states\n",
    "def plot_map(visited_states):\n",
    "    visited_path = np.array([xy(st) for st in visited_states])\n",
    "    visited_unique = np.unique(visited_states)\n",
    "    visited_xy = np.array([xy(st) for st in visited_unique])\n",
    "    visited_counts = np.array([visited_states.count(st) for st in visited_unique])\n",
    "    plt.scatter(visited_xy[:,0], visited_xy[:,1], s=visited_counts*100)\n",
    "    plt.plot(visited_path[:,0], visited_path[:,1], 'k:', alpha=0.5)\n",
    "    plt.xlim(-1,4); plt.ylim(-1,4); plt.xticks([]); plt.yticks([])\n",
    "    for st in states.flatten():\n",
    "        plt.annotate(st, xy(st))\n",
    "        \n",
    "##############################\n",
    "\n",
    "# plot visited states\n",
    "plot_trials = [0, 19, 59, 79, 99]\n",
    "fig = plt.figure(figsize=(18,10))\n",
    "for i in range(len(plot_trials)):\n",
    "    ax = fig.add_subplot(1, len(plot_trials), i+1)\n",
    "    plot_map(visited_states[plot_trials[i]])\n",
    "    ax.set_title('trial ' + str(plot_trials[i]+1))\n",
    "    ax.set_aspect(1)\n",
    "plt.show()\n",
    "\n",
    "# plot Qvalues over trials\n",
    "fig, axes = plt.subplots()\n",
    "s = 7; a = 3 # here you can choose which Q value to plot\n",
    "plt.plot(Q_meta[:,s,a])\n",
    "plt.xlabel('trials')\n",
    "plt.ylabel('Q value({},{})'.format(s,a))\n",
    "plt.show()\n",
    "\n",
    "# plot prediction errors\n",
    "fig, axes = plt.subplots()\n",
    "plt.plot(pred_error_meta,'g')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('prediction error')\n",
    "plt.show()\n",
    "\n",
    "# plot steps\n",
    "fig, axes = plt.subplots()\n",
    "plt.plot(steps,'r')\n",
    "plt.xlabel('trials')\n",
    "plt.ylabel('steps')\n",
    "plt.show()\n",
    "\n",
    "# plot terminal states\n",
    "fig, axes = plt.subplots()\n",
    "plt.plot(s_term_meta,'mx')\n",
    "plt.xlabel('trials')\n",
    "plt.ylabel('terminal state')\n",
    "plt.yticks(s_term)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UARTN4KiAn1N"
   },
   "source": [
    "### 4. Homework exercises\n",
    "\n",
    "> ***Homework exercise 1:*** Describe and explain the evolution of (1) Q-values **(1pt)**, (2) prediction errors **(1pt)** and (3) step number over time **(1pt)** with the given parameter values.\n",
    ">\n",
    "> Hint: You can select which Q-value to plot in the code. Check on what the final value Q-values converge to depends on. What is the highest possible Q-value?\n",
    "\n",
    "> ***Homework exercise 2:*** Play around with the parameter values of the model: $\\alpha$, $\\gamma$, and $\\tau$. Describe and explain the effect of each parameter on the behavior of the agent. **(3pt)**\n",
    "    \n",
    "> ***Homework exercise 3:*** With the original parameter values ($\\alpha = 0.1, \\gamma = 0.5, \\tau = 0.2$), does the agent reach one of the terminal states more often than the other? If so, why is that? How is this affected by the value of the parameters? **(0.5pt)**\n",
    "\n",
    "> ***Homework exercise 4:*** In how far does the behavior of our Q-learning agent differ from what you would expect from a human agent solving the same task (assuming that she does not know the strucutre of the maze, location of terminal states and size of rewards)? Can you think of ways to overcome the shortcomings of the Q-learning algorithm on the given task? **(1pt)**\n",
    "\n",
    "> ***Homework exercise 5:*** What happens if you change the size of the rewards? Try make them negative too. **(0.5pt)**\n",
    "\n",
    "> ***Homework exercise 6:*** Let’s imagine we would design an experiment with 20 human subjects using the current task. Come up with one hypothetical research question you could answer by fitting the  model we implemented (or variants of it) to the behavioral data. **(1pt)**\n",
    "\n",
    "> ***Homework exercise 7:*** In the lecture we discussed how hidden states recovered by model fitting (e.g. prediction errors) can be combined with neural data (e.g. fMRI). Let’s say we conduct a reinforcement learning task in an fMRI scanner with 20 subjects. When analysing our data, we find a brain region in which activity correlates with the prediction errors we computed by fitting a reinforcement learning model to the subject’s behavior. What can (and can’t) we conclude from this? **(1pt)**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
